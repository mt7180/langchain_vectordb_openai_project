{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate local data into openai knowledge base via vector-db\n",
    "## Part I: txt-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector databases are a powerful and emerging class of databases engineered to manage and process structured data in a highly efficient way. They achieve this by indexing and storing vector embeddings, allowing for fast data retrieval. In this context, each data point is depicted as a numerical vector (embedding), making it well-suited for mathematical operations and analysis through machine learning algorithms.\n",
    "\n",
    "These databases empower vector-based search, also known as semantic search, not by relying on exact keyword matching, but by considering the actual meaning of the query. Through the encoding of datasets into meaningful vector representations, the distance between vectors reflects the similarities between the elements. Utilizing algorithms like Approximate Nearest Neighbor (ANN), they enable rapid retrieval of results that closely match the query, facilitating efficient and precise searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vector database](https://miro.medium.com/v2/resize:fit:640/format:webp/0*d8Utelp6ffNhi_eY.png)\n",
    "\n",
    "Source: https://odsc.medium.com/a-gentle-introduction-to-vector-search-3c0511bc6771"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import librarys and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import OpenAI\n",
    "#from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Reference:\n",
    "- [openai](https://platform.openai.com/docs/api-reference?lang=python)\n",
    "- [langchain document_loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n",
    "- [langchain agents](https://python.langchain.com/docs/modules/agents/)\n",
    "- [FAISS vector database](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.faiss.FAISS.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents, make embeddings and put them into a vector database\n",
    "- load documents into memory with <span style=\"color:#40E0D0\">langchain.document_loaders</span>\n",
    "- chunk documents into text pieces of given length and with given overlap to garantee for meaningful context with <span style=\"color:#40E0D0\">langchain.text_splitter.RecursiveCharacterTextSplitter</span>\n",
    "- create word embeddings in the form of embedding vectors for the given text with <span style=\"color:#40E0D0\">langchain.embeddings.OpenaiEmbeddings</span>\n",
    "- load the vectors into a vector database (FAISS) with <span style=\"color:#40E0D0\">langchain.vectorstores</span>\n",
    "- pickle the db for re-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 768.61it/s]\n"
     ]
    }
   ],
   "source": [
    "document_loader = DirectoryLoader('./data', glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True)\n",
    "docs = document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='martin miller was born in 1960 and is male. He works as a software engineer at Google and likes to eat spaghetti and pudding. He goes to bed at 8pm and wakes up at 6am. He loves skiing and knitting. But of cource his greatest passion is writing good, concise and clean python code.', metadata={'source': 'data/example.txt'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "splitted_docs = text_splitter.split_documents(docs)\n",
    "splitted_docs[0] # show first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb_for_txt = FAISS.from_documents(splitted_docs, embeddings)\n",
    "\n",
    "with open('vectordb_for_txt.pkl', 'wb') as file:\n",
    "    pickle.dump(vectordb_for_txt, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Query including vector database and query-history\n",
    "- load pickeled vectore database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('vectordb_for_txt.pkl', 'rb') as pickled_file:\n",
    "    vectorstore = pickle.load(pickled_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    model_name=\"text-davinci-003\",\n",
    "    temperature=0.7,\n",
    "    openai_api_key=API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt_template = \"\"\"The following is a conversation with an AI research assistant. \n",
    "#The assistant tone is technical and scientific.The assistant answers should be easy to understand.\n",
    "#\"\"\"\n",
    "prompt_template = \"\"\"The following is a conversation with an AI research assistant. \n",
    "The assistant tone is technical and scientific.\n",
    "\n",
    "{context}\n",
    "\n",
    "User Question: {question}\n",
    "Answer AI Assistant: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key='answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    memory=chat_history,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    combine_docs_chain_kwargs={'prompt': PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How old is martin miller?',\n",
       " 'chat_history': [HumanMessage(content='How old is martin miller?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=' Martin Miller is 60 years old.', additional_kwargs={}, example=False)],\n",
       " 'answer': ' Martin Miller is 60 years old.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa({'question': 'How old is martin miller?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What does he love?',\n",
       " 'chat_history': [HumanMessage(content='How old is martin miller?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=' Martin Miller is 60 years old.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What does he love?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=' Martin Miller loves skiing, knitting, and writing good, concise, and clean Python code.', additional_kwargs={}, example=False)],\n",
       " 'answer': ' Martin Miller loves skiing, knitting, and writing good, concise, and clean Python code.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa({'question': 'What does he love?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://github.com/Coding-Crashkurse/LangChain-Basics/blob/main/basics.ipynb\n",
    "- https://python.langchain.com/docs/integrations/toolkits/document_comparison_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
